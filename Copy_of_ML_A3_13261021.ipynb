{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of ML_A3_13261021.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vBUIX7IxD3MM",
        "colab_type": "text"
      },
      "source": [
        "# Research Question\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QrJ7Lz9kD6tg",
        "colab_type": "text"
      },
      "source": [
        "*“Question 2: Ensemble methods have been very successful in building classifiers. The hot topics include how to create diverse classifiers and how to fuse the decisions from individual classifiers, in particular how to establish the weights that individual classifiers contribute to the ensemble’s answer. Describe two existing approaches to solving this problem, discuss their advantages and disadvantages. Make a plan to address one issue or two (related to learning the weights or creating diverse classifiers), briefly describe your new method. Explain the reason why the developed method could outperform the conventional ones.)*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ORo9rDIYExOU",
        "colab_type": "text"
      },
      "source": [
        "#Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4XLz5GtQE0n9",
        "colab_type": "text"
      },
      "source": [
        "In context of machine learning, there is no doubt that ensemble methods effectively produce the high-performance predictive model and optimize the results (Smolyakov 2017). By combining a number of machine learning techniques into one ultimate model, noise, variance and bias can be efficiently reduced to enhance the overall performance.The method has been widely applied in domain of data mining and machine learning and it has become one of the most handy strategies to derive the winning solutions in learning competitions or real-world problems (Gutierrez 2014).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SGhh3bma1zIN",
        "colab_type": "text"
      },
      "source": [
        "Although the robustness and performance are enhanced in ensemble methods, the challenges are also addressed. One of the most common issues that data scientists encounter is how contributed weights are adjusted on the individual classifiers when producing the ensemble model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Nv7fCbI12NH",
        "colab_type": "text"
      },
      "source": [
        "The following report will describe two ensemble methods of Bagging and Stacking that involve voting process as the existing approaches and compare the advantages and disadvantages in respect to different aspects. The new method will be made from scrach and introduced to address the issues in the existing approaches. The comparative study of the new method and the conventional solutions will be conducted, the advantages of the new technique that outperform the conventional solutions will be further explained."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yn1jasdKGDef",
        "colab_type": "text"
      },
      "source": [
        "#Ensemble model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FP9MrlOIyW6V",
        "colab_type": "text"
      },
      "source": [
        "![](https://www.analyticsvidhya.com/wp-content/uploads/2015/08/Screen-Shot-2015-08-22-at-6.09.10-pm.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n_3VcDe1FNI5",
        "colab_type": "text"
      },
      "source": [
        "Ensemble methods are trendy and powerful to improve the accuracy of the base models. The idea behind ensemble models is to combine multiple weak models and train them into a more robust classifier to solve the same machine learning task. (Dietterich, 2000) The ensemble model can achieve higher accuracy and robustness than a single classifier with tuned parameters when the classifier memebers are diverse and accurate.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sd-yAWmH14xc",
        "colab_type": "text"
      },
      "source": [
        "As figure shows below, each predictive classifier has its own unique features and performance towards the problem, the supervised learning strategy will be constructed by treating the output from the single classifiers as input to produce a final learner."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ohhZW2iu16ky",
        "colab_type": "text"
      },
      "source": [
        "Although the idea of ensemble moedels is practical and inspiring, there are many challenges that data scientists face when it comes to implementation. Before we fit the output derived from the single classifiers into our stronger learner, the evaluation should be performed to determine the contributed weight of each base model. To optimize the performance as well as to reduce the error of the ensemble model, weighing becomes critical and challenging as we need to take the variety of different base models, complexity and training time of the ensemble model and overall performance into account. The following section will introduce two popular ensemble methods and explain how they approach to solve the issue of weighing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qm99XDVMGHDw",
        "colab_type": "text"
      },
      "source": [
        "##Bagging"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "21yaYnsQ3-Xd",
        "colab_type": "text"
      },
      "source": [
        "![](https://miro.medium.com/max/1920/1*DFHUbdz6EyOuMYP4pDnFlw.jpeg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NdJZkyAUGSKK",
        "colab_type": "text"
      },
      "source": [
        "Bagging comprises of bootstrapping and aggregating which involves weighing the ensemble vote on each base model equally (DeFilippi 2018). It is one of the widely used ensemble methods where it trains multiple models of the same algorithm with subsets of the dataset that are randomly picked from the original training set and take votes on each bag. Since the data in bootstrap samples are randomly selected, the subsets are observed samples distributions from an underlying true distribution, the fitted model varies for each observed sample and therefore the model variance can be improved (Rocca 2019). \n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l1xBnsFK18rM",
        "colab_type": "text"
      },
      "source": [
        "The weak models learn from each other in parallel independently, the final scores for the base models to take into voting process are the averaged scores (Rocca 2019). The Voting classifier in Sklearn library allows us to vote on the outputs that each class has produced and determine the final output (Dietterich 2000)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e1qy4LsN1-jA",
        "colab_type": "text"
      },
      "source": [
        "Classifier voting mechanism can be categorized into two types: hard voting and soft voting. Hard voting refers to returning the classifier that receives the most votes by the ensemble model while soft voting determines the final class based on the model’s average probability (DeFilippi 2018). Both types of voting can achieve higher accuracy than the single classifiers while soft voting only applies when the single classifiers are able to compute the probabilities for the predictions (Zhang et al 2014)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fVlKWQKP2HZq",
        "colab_type": "text"
      },
      "source": [
        "###Advantages"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Q1jH08C4LD_",
        "colab_type": "text"
      },
      "source": [
        "* **Bootstrapping helps decrease the variance of the classifier effectively and avoid overfitting**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uMnw7Y6Z4Omz",
        "colab_type": "text"
      },
      "source": [
        "Bootstrapping allows algorithms to train on multiple subsamples of the original dataset and therefore reduce the chances of overfitting. Bagging is highly efficient when limited data is provided, the estimated scores can be obtained by aggregation from many samples and hence the overall variance can be reduced."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lQJJnvU37NYJ",
        "colab_type": "text"
      },
      "source": [
        "* **parallel ensemble**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5jXopvty7GAw",
        "colab_type": "text"
      },
      "source": [
        "Bagging can apply intensive parallelisation since the base models are trained independently from each other, hence it's more time efficient."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kHZfG8Ri2J-a",
        "colab_type": "text"
      },
      "source": [
        "###Disadvantages"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tD0hUN6j8Zxm",
        "colab_type": "text"
      },
      "source": [
        "* **inefficient to reduce bias or increase predictive force**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JPcorNpB9GdO",
        "colab_type": "text"
      },
      "source": [
        "Bagging combines multiple base models and produces higher stability and a lower variance output than the output that single models can produce. In this case, bias can not be reduced if the base models have high bias initially and the predictive force fails to increase"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qKJIQ5qC9v5d",
        "colab_type": "text"
      },
      "source": [
        "* **reduces robustness of stable base models**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vu18i4UQ8Zr-",
        "colab_type": "text"
      },
      "source": [
        "Variance reduction works most efficiently when error terms in base models are uncorrelated, hence the performance of KNN algorithm and other stable methods can be discredited."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gkh7-SAKGJtK",
        "colab_type": "text"
      },
      "source": [
        "##Stacking"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Nn6g4LEGWhM",
        "colab_type": "text"
      },
      "source": [
        "Unlike bagging, the ensemble method Stacking considers the predictions from the composite base models as input and weighs the contributions of each input conditionally to produce an optimized final prediction (DeFilippi 2018). Stacking upgrades boosting as it doesn’t merely apply an empirical formula to adjust the contributed weight from single classifiers, the meta learner acts as another model to estimate the output from single classifiers and adjust their weights accordingly. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O4S2eQqnAxZo",
        "colab_type": "text"
      },
      "source": [
        "![](https://miro.medium.com/max/1892/0*GHYCJIjkkrP5ZgPh.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d9mBc0_12B2B",
        "colab_type": "text"
      },
      "source": [
        "As shown above, the procedure of stacking can be generalized into 2 learners:\n",
        "* Base Learner: Training data is trained on the single classifier to produce individual output\n",
        "* Meta learner: the collective individual output from base learners becomes input for the meta learner, meta learner can be any learning algorithm to make final predictions on the input"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O5k3cHNPCTrB",
        "colab_type": "text"
      },
      "source": [
        "The conventional way of averaging the contributed weights of each single classifier can be achieved by applying simple linear regression model as the meta learner, this works similarly to minimize variance. Instead, stacking can perform adjustments on those weights to optimize the ensemble model (Charan 2017). More complex models including neural networks, logistic regression or XGBoost can be implemented to optimize the contributed weights to achieve better performance. For example, as Srivastava (2015) stated, the hidden node in neural networks can interconnect the input variable to the output node, the appropriate weight adjustment can be derived from this operation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gn45Q8ZLEo33",
        "colab_type": "text"
      },
      "source": [
        "###Advantages"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "auyYlE86GflL",
        "colab_type": "text"
      },
      "source": [
        "* **Reduce both variance and bias**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j-WfRyKGNYRp",
        "colab_type": "text"
      },
      "source": [
        "Instead of collecting output from weak learners, the base learners in Stacking can be composite and diverse algorithms, the weights can be intuitively learned and adjusted by the learning algorithm in meta learner to approriately adjust weights based on the performance of the single classifier, which leads to better performance and robustness."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dRzKAfl_Ltb3",
        "colab_type": "text"
      },
      "source": [
        "* **increase predictive force**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ss0vqKUxMIj4",
        "colab_type": "text"
      },
      "source": [
        "As mentioned above, the predictive power in stacking tend to increase by combining output from heterogenous base models and computing contruibuted weights using learning algorithms."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ySiDHqCEpmV",
        "colab_type": "text"
      },
      "source": [
        "###Disadvantages"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vHIBxCsEOqon",
        "colab_type": "text"
      },
      "source": [
        "* **overfitting and leakage**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-FRQ1bK5O_xm",
        "colab_type": "text"
      },
      "source": [
        "Unlike ensemble method of Bagging, Stacking allows the base models to train the original dataset individually then pass on the output to the meta learner, which will likely encouter overfitting. Soni (2017) Leakage is also the common issue with Stacking. Since the test set is fit into the base models that have fitted the training set directly, there are high possibilities that the information is shared between the training set and the test set, which eventually leads to test results that are unrealistically accurate. As the result, the model will perform badly on the new dataset that has not seen before. Cross-validation or other validation methods are required to prevent this issue."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QwDaUpCpOvOc",
        "colab_type": "text"
      },
      "source": [
        "* **complexity**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XtItOmKxPevn",
        "colab_type": "text"
      },
      "source": [
        "Stacking tends to combine more complex and diverse base models and then fit the output to the meta learner, the complexity of the ensemble model increases, which requires a lot more computing memories and time to complete training."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uYmPFf7NQJTK",
        "colab_type": "text"
      },
      "source": [
        "#A better approach"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9at6j8PXQUc4",
        "colab_type": "text"
      },
      "source": [
        "After evaluating the pros and cons of different ensemble methods, it can be evident that different approaches have their own beneficial area of domain. It is critical to understand the domain of the problem scenario before we implement any ensemble model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nl_M8YIjRmhU",
        "colab_type": "text"
      },
      "source": [
        "The following section will explore a new appoach to increase the robustness of ensemble models by combining both bagging and stacking methods, and how the new method effectively overcome the issues that the single ensemble method encounters, as well as the further improvement."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iQDr-ArYSxsG",
        "colab_type": "text"
      },
      "source": [
        "##Initial Plan"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jv1JaJgyS1dm",
        "colab_type": "text"
      },
      "source": [
        "The goal of the new approach is to combine Bagging and Stacking in one ensemble model to improve robustness and accuracy of the final prediction. In this way we can produce a more powerful algorithm with reduced variance and bias, and increased predictive force. The new algorithm gains more robustness than a single ensemble method can achieve."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ySH88sr2jgNa",
        "colab_type": "text"
      },
      "source": [
        "![](https://raw.githubusercontent.com/daisychenkeyu/Machine_Learning_A3/master/plan.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TA90nqrJem9n",
        "colab_type": "text"
      },
      "source": [
        "**Step 1**\n",
        "Resample the training set into 5 subsets that contain 100 Boostrapping samples each\n",
        "\n",
        "**Step 2**\n",
        "Train 5 different robust machine learning algorithms on the individual subsets separately to obtain different outputs\n",
        "\n",
        "**Step 3**\n",
        "Perform soft voting on the bags of the output from the single classifier and pruduce a final output for each classifier\n",
        "\n",
        "**Step 4**\n",
        "Consider each output of the single classifier as input for the meta learner, fit the weights from the single classifier into the hidden state of Neural Network\n",
        "\n",
        "**Step 5**\n",
        "Let the Neural Network automatically adjust and opimize the input weights and produce the final ensemble model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LcKR848y1YAk",
        "colab_type": "text"
      },
      "source": [
        "As shown in the above figure, the training set is resampled into 5 subsets of 100 boostrap samples for each base model to train. The final prediction of each model is determined via soft voting which examines the model's average probability (Hsu, K 2013). \n",
        "\n",
        "The vote of each classifier then become the input for the meta learner, which follows the convention of Stacking. The meta learner applies Neural Network to automatically operate weight optimization, the appropriate weight combination will be delivered from the input nodes to the output node directly to produce the final ensemble model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "omU1w7DZ_EnR",
        "colab_type": "text"
      },
      "source": [
        "##Advantages"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_YlOaMIKBFAu",
        "colab_type": "text"
      },
      "source": [
        "The new approach combines two ensemble methods of Bagging and Stacking, which effectively reduces both variance and bias. It overcomes the limitation of reducing variance only in single Bagging method. Since Bagging generally uses weak learner as base models and the confidence level of improving performance of stable algorithms such as KNN is relatively low, the new method implements features of Stacking that allows complex and diverse base learners to train on the first level which efficiently increases the predictive force.\n",
        "\n",
        "The implementation of bagging and average weighting on the output prevents overfitting on the base models and produce the better prediction via soft voting than a fine-tuned single classifier (Güneş 2017). The output of each single classifier then becomes the input to fit in the meta learner.\n",
        "\n",
        "The contributed weight of the single classifier is appropriately adjusted by Neural Network. Ideally the better performance will be assigned more weights."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "06Ndi88X_Iv_",
        "colab_type": "text"
      },
      "source": [
        "##Drawback"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I4ZgzKKhcLYw",
        "colab_type": "text"
      },
      "source": [
        "Although parallelism still applies when determining the final prediction for each classifier, the complexity of the whole model is increased which leads to reduction in model interpret ability. The implementation of Neural Network as the meta learner which increase the amount of computation memories and time. The method might not be practical for time-sensitive applications."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x9HomaE1mdSX",
        "colab_type": "text"
      },
      "source": [
        "##Validation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vgx5HCtOem2Z",
        "colab_type": "text"
      },
      "source": [
        "Overfitting becomes the unavoidable concern for this new approach. To prevent overfitting, k-fold cross-validation should be implemented on the test set accordingly.\n",
        "\n",
        "As mentioned above, overfitting and leakage lead to unrealistic accuracy once the information is accidently shared between training set and test set. The idea of the k-fold cross-validation is to train the model on the partial training set and observe the performance of the trained model on the remaining part of the training set. In this way, overfitting can be detected."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NsDPV39I_LX4",
        "colab_type": "text"
      },
      "source": [
        "##Further improvement"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O8J2HhqtqZQq",
        "colab_type": "text"
      },
      "source": [
        "* **Hyperparameter Tuning**\n",
        "\n",
        "Shahhosseini et al.(2019) stated that the hyperparameters tuning for each of the single classifier can be done during cross-validation, the models can be trained and tuned in parellel computing which makes the process more time efficient. Alternatively, hyperparameter optimizations can be done automatically when implementing the single classifiers (Mohsen, S et al 2019). \n",
        "\n",
        "* **Regularization**\n",
        "\n",
        "As Brownlee (2019) stated, regularization techniques can be applied on the neural network to avoid overfitting and reduce generalization errors of the ensemble model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eFocm9kjvlwi",
        "colab_type": "text"
      },
      "source": [
        "#Conclusion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8KwxwtyHAPJb",
        "colab_type": "text"
      },
      "source": [
        "Ensemble models have attracted attentions in area of machine learning, they are commonly used in many real-life problem solving tasks and machine learning competitions. By implementing ensemble methods, the intelligence of multiple classifiers can be combined to reduce bias, variance and noise. The robustness of ensemble models can be applied in many domain of machine learning tasks including classification, regression, clusterring and other problems. \n",
        "\n",
        "Just like many powerful machine learning algorithms, ensemble models also have their drawbacks. It is essential to understand the domain of the research and apply ensemble methods in the appropriate context."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V0yq93YXvp8P",
        "colab_type": "text"
      },
      "source": [
        "#Reference"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iMqWL5h8Eipc",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "Brownlee 2019, *How to Avoid Overfitting in Deep Learning Neural Networks*, Machine Learning Mastery, viewed 6 October 2019, < https://machinelearningmastery.com/introduction-to-regularization-to-reduce-overfitting-and-improve-generalization-error/ >\n",
        "\n",
        "Charan 2017, *Stacking — A Super Learning Technique*, Medium, viewed 4 October 2019, < https://medium.com/@gurucharan_33981/stacking-a-super-learning-technique-dbed06b1156d >\n",
        "\n",
        "Srivastava 2015, *Finding Optimal Weights of Ensemble Learner using Neural Network*, Analyticsvidhya, viewed 7 October 2019, < https://www.analyticsvidhya.com/blog/2015/08/optimal-weights-ensemble-learner-neural-network/ >\n",
        "\n",
        "DeFilippi 2018, *Boosting, Bagging, and Stacking — Ensemble Methods with sklearn and mlens*, Medium, < https://medium.com/@rrfd/boosting-bagging-and-stacking-ensemble-methods-with-sklearn-and-mlens-a455c0c982de >\n",
        "\n",
        "Dietterich, 2000, *Ensemble Methods in Machine Learning*, Springer, Berlin, Heidelberg, vol. 1857, December, pp. 1-15\n",
        "\n",
        "Gutierrez 2014, *Ask a Data Scientist: Ensemble Methods*, Inside Big Data, viewed 4 October, < https://insidebigdata.com/2014/12/18/ask-data-scientist-ensemble-methods/ >\n",
        "\n",
        "Güneş, 2017, *Why do stacked ensemble models win data science competitions?*, SAS, viewed, 6 October, < https://blogs.sas.com/content/subconsciousmusings/2017/05/18/stacked-ensemble-models-win-data-science-competitions/ >\n",
        "\n",
        "Hsu, K. 2013, *Weight-Adjusted Bagging of Classification Algorithms\n",
        "Sensitive to Missing Values *, International Journal of Information and Education Technology, Vol. 3, No. 5, pp. 560-566\n",
        "\n",
        "Mohsen, S., Guiping, H & Pham, H. 2019, *Optimizing Ensemble Weights and Hyperparameters of Machine Learning Models for Regression Problems*, Cornell University Library, arXiv.org\n",
        "\n",
        "Rocca 2019, *Ens\n",
        "emble methods: bagging, boosting and stacking*, Towards Science, viewed 4 October 2019, < https://towardsdatascience.com/ensemble-methods-bagging-boosting-and-stacking-c9214a10a205 >\n",
        "\n",
        "Smolyakov 2017, *Ensemble Learning to Improve Machine Learning Results*, Stats and Bots, viewed 4 October, < https://blog.statsbot.co/ensemble-learning-d1dcd548e936 >\n",
        "\n",
        "Soni 2017, *Data Leakage in Machine Learning*, Towards Data Science, viewed 6 October 2019, \n",
        "< https://towardsdatascience.com/data-leakage-in-machine-learning-10bdd3eec742 >\n",
        "\n",
        "Zhang, Y., Zhang, H., Cai, J & Yang, B. 2014, *A Weighted Voting Classifier Based on Differential Evolution*, Abstract and Applied Analysis, Vol 2014, April, pp. 1-6\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ]
}